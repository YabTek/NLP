{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5azP0mxLarxr"
      },
      "source": [
        "**#1 N-gram language model**\n",
        "\n",
        "**1.1 Create n-grams for n=1, 2, 3, 4. You can show sample prints**                                                        \n",
        "\n",
        "*   In order to create n-grams, first I have to read the text file, which is the given Amharic corpus  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCHaCghq-Oho"
      },
      "outputs": [],
      "source": [
        "corpus = open('GPAC.txt', 'r', encoding='utf-8', errors = 'ignore').read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znot_6qEFdtf"
      },
      "source": [
        "\n",
        "\n",
        "*   Then let's start creating n-grams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_cTJtBtI2pv",
        "outputId": "b71bf0a4-370c-4deb-e0e4-ff303ee06a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#install natural language toolkit\n",
        "!pip install nltk\n",
        "\n",
        "#download punkt\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn1r1CxgFmid",
        "outputId": "a71e0f52-82df-4aed-8d04-006c8cd877db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigrams--> [('ምን',), ('መሰላችሁ',), ('አንባቢያን',)]\n",
            "Bigrams--> [('ምን', 'መሰላችሁ'), ('መሰላችሁ', 'አንባቢያን'), ('አንባቢያን', 'ኢትዮጵያ')]\n",
            "Trigrams--> [('ምን', 'መሰላችሁ', 'አንባቢያን'), ('መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ'), ('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ')]\n",
            "Fourgrams--> [('ምን', 'መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ'), ('መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'), ('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው')]\n"
          ]
        }
      ],
      "source": [
        "#import the necessary libraries\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#tokenize the text into words\n",
        "words = word_tokenize(corpus)\n",
        "\n",
        "#list of possible punctuations in the corpus\n",
        "punctuations = {'፡', '።', '፣', '፤', '፥', '፦', '፧',  '፤፤', '.' ,'(', ')', '?', '::','!'}\n",
        "\n",
        "#remove punctuations from the corpus\n",
        "cleaned_words = []\n",
        "\n",
        "for word in words:\n",
        "    if any(char in punctuations for char in word):\n",
        "        cleaned_word = ''.join(char for char in word if char not in punctuations)\n",
        "        if cleaned_word:\n",
        "            cleaned_words.append(cleaned_word)\n",
        "    else:\n",
        "        cleaned_words.append(word)\n",
        "\n",
        "#define a function to create n-grams\n",
        "def create_ngrams(n):\n",
        "  ngram = list(ngrams(cleaned_words, n))\n",
        "  return ngram\n",
        "\n",
        "#call a function and show sample prints\n",
        "unigrams = create_ngrams(1)\n",
        "print(\"Unigrams-->\", unigrams[:3])\n",
        "\n",
        "bigrams = create_ngrams(2)\n",
        "print(\"Bigrams-->\", bigrams[:3])\n",
        "\n",
        "trigrams = create_ngrams(3)\n",
        "print(\"Trigrams-->\", trigrams[:3])\n",
        "\n",
        "four_grams = create_ngrams(4)\n",
        "print(\"Fourgrams-->\", four_grams[:3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVojLu59l6Hp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgrwxRAaOsSb"
      },
      "source": [
        "**1.2 Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACd7fqKZOwhA",
        "outputId": "89b8af98-5d9d-4fc8-d1d7-14080cb14cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 Unigrams--> [[('ነው',), 0.016446757837111477], [('ላይ',), 0.008632521757771754], [('ወደ',), 0.004956772032930116], [('ነበር',), 0.00485219878329024], [('ግን',), 0.004792069164747312], [('እና',), 0.004609065977877529], [('ውስጥ',), 0.004342404191295846], [('ጋር',), 0.0038718245679164043], [('ነገር',), 0.00333588666351204], [('አንድ',), 0.003192098445257211]]\n",
            "Top 10 Bigrams--> [[('ዓ', 'ም'), 0.0014169712370524906], [('ነገር', 'ግን'), 0.0007424720135108992], [('ብቻ', 'ሳይሆን'), 0.0005490109959059465], [('ማለት', 'ነው'), 0.0005359393055272335], [('ብቻ', 'ነው'), 0.0005124102628455502], [('አዲስ', 'አበባ'), 0.00034247828792228097], [('ምን', 'ያህል'), 0.00030587755486188455], [('ኤ', 'አ'), 0.0002980345406346567], [('እ', 'ኤ'), 0.0002771198360287159], [('ሚሊዮን', 'ዶላር'), 0.00024574777911980463]]\n",
            "Top 10 Trigrams--> [[('እ', 'ኤ', 'አ'), 0.00027450621560502476], [('2004', 'ዓ', 'ም'), 0.00012025986588410609], [('በ2003', 'ዓ', 'ም'), 0.0001098024862420099], [('ነው', 'ነገር', 'ግን'), 8.10446922262454e-05], [('ቀን', '2004', 'ዓ'), 8.10446922262454e-05], [('2003', 'ዓ', 'ም'), 7.320165749467327e-05], [('ነበር', 'ነገር', 'ግን'), 7.320165749467327e-05], [('ዓ', 'ም', 'ጀምሮ'), 6.0129932942053045e-05], [('ሀሁ', 'በስድስት', 'ወር'), 5.228689821048091e-05], [('በ2004', 'ዓ', 'ም'), 5.228689821048091e-05]]\n",
            "Top 10 Fourgrams--> [[('ቀን', '2004', 'ዓ', 'ም'), 7.843055236023675e-05], [('በብርሃንና', 'ሰላም', 'ማተሚያ', 'ቤት'), 2.8757869198753475e-05], [('አዲስ', 'አበባ', 'ስፖርት', 'ማህበር'), 2.8757869198753475e-05], [('ጋር', 'ሆኖ', 'ኢህአዴግን', 'ወጋ'), 2.6143517453412252e-05], [('ነው', 'የአለም', 'የኢኮኖሚ', 'ቀውስና'), 2.6143517453412252e-05], [('የአለም', 'የኢኮኖሚ', 'ቀውስና', 'የነዳጅ'), 2.6143517453412252e-05], [('የኢኮኖሚ', 'ቀውስና', 'የነዳጅ', 'ዋጋ'), 2.6143517453412252e-05], [('ቀን', '2003', 'ዓ', 'ም'), 2.3529165708071025e-05], [('ከዚህ', 'በኋላ', 'ነው', 'እንግዲህ'), 2.3529165708071025e-05], [('ዓ', 'ም', 'እ', 'ኤ'), 2.09148139627298e-05]]\n"
          ]
        }
      ],
      "source": [
        "#import frequency distribution class\n",
        "from nltk import FreqDist\n",
        "\n",
        "#define a function to calculate probabilities and find 10 most likely n-grams\n",
        "def calculate(ngrams):\n",
        "  freq_dist = FreqDist(ngrams)\n",
        "\n",
        "  total = sum(freq_dist.values())\n",
        "  probabilities = []\n",
        "\n",
        "  for ngram, freq in freq_dist.items():\n",
        "      probability = freq / total\n",
        "      probabilities.append([ngram, probability])\n",
        "\n",
        "  probabilities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  return probabilities[:10]\n",
        "\n",
        "#top 10 most likely unigrams\n",
        "print(\"Top 10 Unigrams-->\", calculate(unigrams))\n",
        "\n",
        "#top 10 most likely bigrams\n",
        "print(\"Top 10 Bigrams-->\", calculate(bigrams))\n",
        "\n",
        "#top  10 most likely trigrams\n",
        "print(\"Top 10 Trigrams-->\", calculate(trigrams))\n",
        "\n",
        "#top 10 likely fourgrams\n",
        "print(\"Top 10 Fourgrams-->\", calculate(four_grams))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k68T3skgdK3c"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**1.3 What is the probability of the sentence. \"ኢትዮጵያ ታሪካዊ ሀገር ናት \". You can\n",
        "also try more sentences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfy7XvDy5XYq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7aVFYUGdtjE",
        "outputId": "266323e2-5f8a-48fe-c5e5-7733611b28d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The interpolated probability of the sentence 'ኢትዮጵያ ታሪካዊ ሀገር ናት' is: 2.1980542190308178e-10\n"
          ]
        }
      ],
      "source": [
        "sentence = \"ኢትዮጵያ ታሪካዊ ሀገር ናት\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "# Calculate the probabilities using interpolation\n",
        "def calculate(words, ngrams):\n",
        "    # Set interpolation weights\n",
        "    lambdas = [0.4, 0.3, 0.2, 0.1]\n",
        "\n",
        "    # Initialize probability\n",
        "    probability = 1.0\n",
        "\n",
        "    # Loop through each token in the sentence\n",
        "    for i in range(len(words)):\n",
        "        # Get the n-gram context\n",
        "        context = words[0:i+1]\n",
        "\n",
        "        # Check if the n-gram context exists in the n-grams\n",
        "        if tuple(context) in ngrams[i]:\n",
        "\n",
        "            # Calculate the conditional probability using the n-gram\n",
        "            conditional_probability = ngrams[i][tuple(context)] / sum(ngrams[i].values())\n",
        "\n",
        "            # Update the overall probability using interpolation\n",
        "            probability *= (lambdas[i] * conditional_probability)\n",
        "\n",
        "    return probability\n",
        "\n",
        "# define list of frequency distributions of the large corpus for the four ngrams\n",
        "freqDistList = [FreqDist(unigrams), FreqDist(bigrams), FreqDist(trigrams), FreqDist(four_grams)]\n",
        "\n",
        "\n",
        "print(f\"The interpolated probability of the sentence '{sentence}' is: {calculate(words, freqDistList)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daE02ji7uSso"
      },
      "source": [
        "**1.4 Generate random sentences using n-grams; explain what happens as n increases, based on your output.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1xhKi32HTuH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqUMnID7uayE",
        "outputId": "9cf12784-1d0e-4117-bcd4-8e38bd975356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def generate_sentence(ngrams, n, length=6):\n",
        "    sentence = []\n",
        "\n",
        "    # Randomly choose a starting n-gram\n",
        "    current_ngram = random.choice(ngrams[n])\n",
        "\n",
        "    # Add the words from the starting n-gram to the sentence\n",
        "    sentence.extend(current_ngram)\n",
        "\n",
        "    # Generate the rest of the sentence\n",
        "    for _ in range(length - n):\n",
        "        # Use the last (n-1) words as the context\n",
        "        context = tuple(sentence[-(n-1):])\n",
        "\n",
        "        # Get the next word based on the context\n",
        "        next_words = [gram[-1] for gram in ngrams[n] if gram[:-1] == context]\n",
        "        next_word = random.choice(next_words) if next_words else None\n",
        "\n",
        "        # Add the next word to the sentence\n",
        "        if next_word:\n",
        "            sentence.append(next_word)\n",
        "        else:\n",
        "            break  # Break if no next word is found based on the context\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "# Example usage for generating sentences with different values of n\n",
        "ngrams = {1: unigrams, 2: bigrams, 3: trigrams, 4: four_grams}\n",
        "for n_value in range(1, 5):\n",
        "    generated_sentence = generate_sentence(ngrams, n_value)\n",
        "    print(f\"Generated sentence with {ngrams[n_value]}:\", generated_sentence)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkofCiRvR8bi"
      },
      "source": [
        "\n",
        "\n",
        "*   **From the output we can conclude, as the value of n increases, we can get a more meaningful and contextually coherent sentence.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxQRrHXdQYSd"
      },
      "source": [
        "### **#2 Evaluate these Language Models Using Intrinsic Evaluation Method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi-rOjZXRRBf",
        "outputId": "738c9e6b-90d0-49f5-c1b2-b4bd25d535fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for n=1: 144.9949783208397\n",
            "Perplexity for n=2: 14.43831658248446\n",
            "Perplexity for n=3: 1.5596752088530137\n",
            "Perplexity for n=4: 1.0804521313169733\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import math\n",
        "import string\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    cleaned_text = text.replace('\\n', ' ').replace('\\r', '')\n",
        "    cleaned_text = cleaned_text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def train_language_model(train_data, n, k):\n",
        "    tokens = nltk.word_tokenize(train_data)\n",
        "    ngrams_list = list(ngrams(tokens, n))\n",
        "    ngrams_freq = Counter(ngrams_list)\n",
        "\n",
        "\n",
        "    model = {}\n",
        "    for ngram, count in ngrams_freq.items():\n",
        "        context = tuple(ngram[:-1])\n",
        "        word = ngram[-1]\n",
        "        if context not in model:\n",
        "            model[context] = {}\n",
        "        model[context][word] = math.log((count + k) / (ngrams_freq[context] + k * len(ngrams_freq)))\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def calculate_perplexity(model, test_data):\n",
        "    test_tokens = nltk.word_tokenize(test_data)\n",
        "    num_words = len(test_tokens)\n",
        "    log_prob_sum = 0.0\n",
        "\n",
        "\n",
        "    for i in range(len(test_tokens)):\n",
        "        if i < n-1:\n",
        "            context = tuple(test_tokens[:i])\n",
        "        else:\n",
        "            context = tuple(test_tokens[i-n+1:i])\n",
        "        word = test_tokens[i]\n",
        "\n",
        "\n",
        "        if context in model and word in model[context]:\n",
        "            log_prob = model[context][word]\n",
        "            log_prob_sum += log_prob\n",
        "\n",
        "\n",
        "    perplexity = math.exp(-log_prob_sum / num_words)\n",
        "\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "with open('/content/GPAC.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    text = file.read()\n",
        "\n",
        "\n",
        "train_size = int(0.8 * len(text))\n",
        "train_data = text[:train_size]\n",
        "test_data = text[train_size:]\n",
        "\n",
        "\n",
        "train_data = preprocess_text(train_data)\n",
        "test_data = preprocess_text(test_data)\n",
        "\n",
        "\n",
        "n_values = [1, 2, 3, 4]\n",
        "k = 0.1\n",
        "\n",
        "\n",
        "for n in n_values:\n",
        "    model = train_language_model(train_data, n, k)\n",
        "    perplexity = calculate_perplexity(model, test_data)\n",
        "\n",
        "    print(f\"Perplexity for n={n}: {perplexity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C3kiXJORC9F"
      },
      "source": [
        "### **#3 Evaluate these Language Models Using extrinsic Evaluation Method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi6gr0x7Sl-s",
        "outputId": "8b6ff841-bb31-485b-e58e-81eaed53b746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigrams accuracy is: 0.0009821327390171118\n",
            "bigrams accuracy is: 0.003298944841306483\n",
            "trigrams accuracy is: 0.016108520559559136\n",
            "fourgrams accuracy is: 0.052774280198102916\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "import codecs\n",
        "# Read the text file\n",
        "file_path = '/content/GPAC.txt'\n",
        "\n",
        "# Read the corpus from the text file with explicit encoding\n",
        "with codecs.open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "def splitData(text, train_ratio=0.8):\n",
        "  # Split the preprocessed text into words using Amharic punctuation marks as separators\n",
        "  words = re.split('[።፥፤፦]+', text)\n",
        "\n",
        "  # Determine the index to split the data\n",
        "  split_index = int(len(words) * train_ratio)\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  train_data = words[:split_index]\n",
        "\n",
        "  test_data = words[split_index:]\n",
        "  return train_data, test_data\n",
        "train_data , test_data = splitData(text)\n",
        "s,e,step = 4,0,-1\n",
        "d = {2:\"trigrams\",4:\"unigrams\",\n",
        "3:\"bigrams\",1:\"fourgrams\"}\n",
        "train_data = ' '.join(train_data)\n",
        "\n",
        "\n",
        "\n",
        "def train_ngram_model(text, n):\n",
        "    ngram_model = defaultdict(list)\n",
        "    words = text.split()\n",
        "\n",
        "\n",
        "    for i in range(len(words) - n):\n",
        "        context = tuple(words[i:i+n])\n",
        "        target_word = words[i+n]\n",
        "        ngram_model[context].append(target_word)\n",
        "\n",
        "    return ngram_model\n",
        "\n",
        "\n",
        "\n",
        "def next_word_prediction(context, ngram_model):\n",
        "    if context not in ngram_model:\n",
        "        return None\n",
        "\n",
        "    possible_words = ngram_model[context]\n",
        "    word_counts = defaultdict(int)\n",
        "    for word in possible_words:\n",
        "        word_counts[word] += 1\n",
        "\n",
        "    total_count = sum(word_counts.values())\n",
        "    probabilities = {word: count / total_count for word, count in word_counts.items()}\n",
        "\n",
        "    predicted_word = max(probabilities, key=probabilities.get)\n",
        "    return predicted_word\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_next_word_prediction(test_data, ngram_model, n):\n",
        "    sentences = test_data.split('።')\n",
        "    total_predictions = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        for i in range(len(words) - n):  # Adjusted loop range\n",
        "            context = tuple(words[i:i+n])\n",
        "            target_word = words[i+n]\n",
        "            predicted_word = next_word_prediction(context, ngram_model)\n",
        "            if predicted_word == target_word:\n",
        "                correct_predictions += 1\n",
        "            total_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy\n",
        "predict_test = ' '.join(test_data)\n",
        "for n in range(s,e,step):\n",
        "#predict_test = ' '.join(test_data)\n",
        "\n",
        "\n",
        "\n",
        "  #In order to check the accuracy\n",
        "  #start predicting for different n values\n",
        "  ngram_model = train_ngram_model(train_data, n)\n",
        "  accuracy = evaluate_next_word_prediction(predict_test, ngram_model, n)\n",
        "  print(f\"{d[n]} accuracy is: {accuracy}\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}